Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that, through cellular respiration, can later be released to fuel the organism's metabolic activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water – hence the name photosynthesis, from the Greek phōs (φῶς), "light", and sunthesis (σύνθεσις), "putting together". In most cases, oxygen is also released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis; such organisms are called photoautotrophs. Photosynthesis is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth. 
Although photosynthesis is performed differently by different species, the process always begins when energy from light is absorbed by proteins called reaction centers that contain green chlorophyll pigments. In plants, these proteins are held inside organelles called chloroplasts, which are most abundant in leaf cells, while in bacteria they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two further compounds that serve as short-term stores of energy, enabling its transfer to drive other reactions: these compounds are reduced nicotinamide adenine dinucleotide phosphate (NADPH) and adenosine triphosphate (ATP), the "energy currency" of cells. 
In plants, algae and cyanobacteria, long-term energy storage in the form of sugars is produced by a subsequent sequence of light-independent reactions called the Calvin cycle. In the Calvin cycle, atmospheric carbon dioxide is incorporated into already existing organic carbon compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose. In other bacteria, different mechanisms such as the reverse Krebs cycle are used to achieve the same end. 
The first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. Today, the average rate of energy capture by photosynthesis globally is approximately 130 terawatts, which is about eight times the current power consumption of human civilization. Photosynthetic organisms also convert around 100–115 billion tons (91–104 petagrams) of carbon into biomass per year. The phenomenon that plants receive some energy from light – in addition to air, soil, and water – was first discovered in 1779 by Jan Ingenhousz. 
n photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded in cell membranes. In its simplest form, this involves the membrane surrounding the cell itself. However, the membrane may be tightly folded into cylindrical sheets called thylakoids, or bunched up into round vesicles called intracytoplasmic membranes. These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb. 
In plants and algae, photosynthesis takes place in organelles called chloroplasts. A typical plant cell contains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral and peripheral membrane protein complexes of the photosynthetic system. 
Plants absorb light primarily using the pigment chlorophyll. The green part of the light spectrum is not absorbed but is reflected which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such as carotenes and xanthophylls. Algae also use chlorophyll, but various other pigments are present, such as phycocyanin, carotenes, and xanthophylls in green algae, phycoerythrin in red algae (rhodophytes) and fucoxanthin in brown algae and diatoms resulting in a wide variety of colors. 
These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex. 
Although all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to reduce heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place. 
Plants usually convert light into chemical energy with a photosynthetic efficiency of 3–6%. Absorbed light that is unconverted is dissipated primarily as heat, with a small fraction (1–2%) re-emitted as chlorophyll fluorescence at longer (redder) wavelengths. This fact allows measurement of the light reaction of photosynthesis by using chlorophyll fluorometers. 
Actual plants' photosynthetic efficiency varies with the frequency of the light being converted, light intensity, temperature and proportion of carbon dioxide in the atmosphere, and can vary from 0. 1% to 8%. By comparison, solar panels convert light into electric energy at an efficiency of approximately 6–20% for mass-produced panels, and above 40% in laboratory devices. 
The efficiency of both light and dark reactions can be measured but the relationship between the two can be complex. For example, the ATP and NADPH energy molecules, created by the light reaction, can be used for carbon fixation or for photorespiration in C3 plants. Electrons may also flow to other electron sinks. For this reason, it is not uncommon for authors to differentiate between work done under non-photorespiratory conditions and under photorespiratory conditions. 
Chlorophyll fluorescence of photosystem II can measure the light reaction, and Infrared gas analyzers can measure the dark reaction. It is also possible to investigate both at the same time using an integrated chlorophyll fluorometer and gas exchange system, or by using two separate systems together. Infrared gas analyzers and some moisture sensors are sensitive enough to measure the photosynthetic assimilation of CO2, and of ΔH2O using reliable methods CO2 is commonly measured in μmols/(m2/s), parts per million or volume per million and H2O is commonly measured in mmol/(m2/s) or in mbars. By measuring CO2 assimilation, ΔH2O, leaf temperature, barometric pressure, leaf area, and photosynthetically active radiation or PAR, it becomes possible to estimate, "A" or carbon assimilation, "E" or transpiration, "gs" or stomatal conductance, and Ci or intracellular CO2. However, it is more common to used chlorophyll fluorescence for plant stress measurement, where appropriate, because the most commonly used measuring parameters FV/FM and Y(II) or F/FM' can be made in a few seconds, allowing the measurement of larger plant populations. 
Gas exchange systems that offer control of CO2 levels, above and below ambient, allow the common practice of measurement of A/Ci curves, at different CO2 levels, to characterize a plant's photosynthetic response. 
Integrated chlorophyll fluorometer – gas exchange systems allow a more precise measure of photosynthetic response and mechanisms. While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO2 levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of CC to replace Ci. The estimation of CO2 at the site of carboxylation in the chloroplast, or CC, becomes possible with the measurement of mesophyll conductance or gm using an integrated system. 
Photosynthesis measurement systems are not designed to directly measure the amount of light absorbed by the leaf. But analysis of chlorophyll-fluorescence, P700- and P515-absorbance and gas exchange measurements reveal detailed information about e. g. the photosystems, quantum efficiency and the CO2 assimilation rates. With some instruments, even wavelength-dependency of the photosynthetic efficiency can be analyzed. 
A phenomenon known as quantum walk increases the efficiency of the energy transport of light significantly. In the photosynthetic cell of an algae, bacterium, or plant, there are light-sensitive molecules called chromophores arranged in an antenna-shaped structure named a photocomplex. When a photon is absorbed by a chromophore, it is converted into a quasiparticle referred to as an exciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form that makes it accessible for the cell's metabolism. The exciton's wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously "choose" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time. 
Because that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances, due to obstacles in the form of destructive interference that come into play. These obstacles cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic "hop". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks. 
n physiology, respiration is the movement of oxygen from the outside environment to the cells within tissues, and the removal of carbon dioxide in the opposite direction. 
The physiological definition of respiration differs from the biochemical definition, which refers to a metabolic process by which an organism obtains energy (in the form of ATP and NADPH) by oxidizing nutrients and releasing waste products. Although physiologic respiration is necessary to sustain cellular respiration and thus life in animals, the processes are distinct: cellular respiration takes place in individual cells of the organism, while physiologic respiration concerns the diffusion and transport of metabolites between the organism and the external environment. 
Gas exchanges in the lung occurs by ventilation and perfusion. Ventilation refers to the in and out movement of air of the lungs and perfusion is the circulation of blood in the pulmonary capillaries. In mammals, physiological respiration involves respiratory cycles of inhaled and exhaled breaths. Inhalation (breathing in) is usually an active movement that brings air into the lungs where the process of gas exchange takes place between the air in the alveoli and the blood in the pulmonary capillaries. Contraction of the diaphragm muscle cause a pressure variation, which is equal to the pressures caused by elastic, resistive and inertial components of the respiratory system. In contrast, exhalation (breathing out) is usually a passive process. 
The process of breathing does not fill the alveoli with atmospheric air during each inhalation (about 350 ml per breath), but the inhaled air is carefully diluted and thoroughly mixed with a large volume of gas (about 2. 5 liters in adult humans) known as the functional residual capacity which remains in the lungs after each exhalation, and whose gaseous composition differs markedly from that of the ambient air. Physiological respiration involves the mechanisms that ensure that the composition of the functional residual capacity is kept constant, and equilibrates with the gases dissolved in the pulmonary capillary blood, and thus throughout the body. Thus, in precise usage, the words breathing and ventilation are hyponyms, not synonyms, of respiration; but this prescription is not consistently followed, even by most health care providers, because the term respiratory rate (RR) is a well-established term in health care, even though it would need to be consistently replaced with ventilation rate if the precise usage were to be followed. 
The kidneys are two reddish-brown bean-shaped organs found in vertebrates. They are located on the left and right in the retroperitoneal space, and in adult humans are about 12 centimetres (4+1⁄2 inches) in length. They receive blood from the paired renal arteries; blood exits into the paired renal veins. Each kidney is attached to a ureter, a tube that carries excreted urine to the bladder. 
The kidney participates in the control of the volume of various body fluids, fluid osmolality, acid–base balance, various electrolyte concentrations, and removal of toxins. Filtration occurs in the glomerulus: one-fifth of the blood volume that enters the kidneys is filtered. Examples of substances reabsorbed are solute-free water, sodium, bicarbonate, glucose, and amino acids. Examples of substances secreted are hydrogen, ammonium, potassium and uric acid. The kidneys also carry out functions independent of the nephron. For example, they convert a precursor of vitamin D to its active form, calcitriol; and synthesize the hormones erythropoietin and renin. The nephron is the structural and functional unit of the kidney. Each adult human kidney contains around 1 million nephrons, while a mouse kidney contains only about 12,500 nephrons. 
Renal physiology is the study of kidney function. Nephrology is the medical specialty which addresses diseases of kidney function: these include chronic kidney disease, nephritic and nephrotic syndromes, acute kidney injury, and pyelonephritis. Urology addresses diseases of kidney (and urinary tract) anatomy: these include cancer, renal cysts, kidney stones and ureteral stones, and urinary tract obstruction. The word “renal” is an adjective meaning “relating to the kidneys”, and its roots are French or late Latin. Whereas according to some opinions, "renal" should be replaced with "kidney" in scientific writings such as "kidney artery", other experts have advocated preserving the use of renal as appropriate including in "renal artery". 
Procedures used in the management of kidney disease include chemical and microscopic examination of the urine (urinalysis), measurement of kidney function by calculating the estimated glomerular filtration rate (eGFR) using the serum creatinine; and kidney biopsy and CT scan to evaluate for abnormal anatomy. Dialysis and kidney transplantation are used to treat kidney failure; one (or both sequentially) of these are almost always used when renal function drops below 15%. Nephrectomy is frequently used to cure renal cell carcinoma. 
In humans, the kidneys are located high in the abdominal cavity, one on each side of the spine, and lie in a retroperitoneal position at a slightly oblique angle. The asymmetry within the abdominal cavity, caused by the position of the liver, typically results in the right kidney being slightly lower and smaller than the left, and being placed slightly more to the middle than the left kidney. The left kidney is approximately at the vertebral level T12 to L3, and the right is slightly lower. The right kidney sits just below the diaphragm and posterior to the liver. The left kidney sits below the diaphragm and posterior to the spleen. On top of each kidney is an adrenal gland. The upper parts of the kidneys are partially protected by the 11th and 12th ribs. Each kidney, with its adrenal gland is surrounded by two layers of fat: the perirenal fat present between renal fascia and renal capsule and pararenal fat superior to the renal fascia. 
The kidney is a bean-shaped structure with a convex and a concave border. A recessed area on the concave border is the renal hilum, where the renal artery enters the kidney and the renal vein and ureter leave. The kidney is surrounded by tough fibrous tissue, the renal capsule, which is itself surrounded by perirenal fat, renal fascia, and pararenal fat. The anterior (front) surface of these tissues is the peritoneum, while the posterior (rear) surface is the transversalis fascia. 
The superior pole of the right kidney is adjacent to the liver. For the left kidney, it is next to the spleen. Both, therefore, move down upon inhalation. 
The functional substance, or parenchyma, of the kidney is divided into two major structures: the outer renal cortex and the inner renal medulla. Grossly, these structures take the shape of eight to 18 cone-shaped renal lobes, each containing renal cortex surrounding a portion of medulla called a renal pyramid. Between the renal pyramids are projections of cortex called renal columns. Nephrons, the urine-producing functional structures of the kidney, span the cortex and medulla. The initial filtering portion of a nephron is the renal corpuscle, which is located in the cortex. This is followed by a renal tubule that passes from the cortex deep into the medullary pyramids. Part of the renal cortex, a medullary ray is a collection of renal tubules that drain into a single collecting duct. 
The tip, or papilla, of each pyramid empties urine into a minor calyx; minor calyces empty into major calyces, and major calyces empty into the renal pelvis. This becomes the ureter. At the hilum, the ureter and renal vein exit the kidney and the renal artery enters. Hilar fat and lymphatic tissue with lymph nodes surround these structures. The hilar fat is contiguous with a fat-filled cavity called the renal sinus. The renal sinus collectively contains the renal pelvis and calyces and separates these structures from the renal medullary tissue. 
The kidneys possess no overtly moving structures. 
In the majority of vertebrates, the mesonephros persists into the adult, albeit usually fused with the more advanced metanephros; only in amniotes is the mesonephros restricted to the embryo. The kidneys of fish and amphibians are typically narrow, elongated organs, occupying a significant portion of the trunk. The collecting ducts from each cluster of nephrons usually drain into an archinephric duct, which is homologous with the vas deferens of amniotes. However, the situation is not always so simple; in cartilaginous fish and some amphibians, there is also a shorter duct, similar to the amniote ureter, which drains the posterior (metanephric) parts of the kidney, and joins with the archinephric duct at the bladder or cloaca. Indeed, in many cartilaginous fish, the anterior portion of the kidney may degenerate or cease to function altogether in the adult. 
In the most primitive vertebrates, the hagfish and lampreys, the kidney is unusually simple: it consists of a row of nephrons, each emptying directly into the archinephric duct. Invertebrates may possess excretory organs that are sometimes referred to as "kidneys", but, even in Amphioxus, these are never homologous with the kidneys of vertebrates, and are more accurately referred to by other names, such as nephridia. In amphibians, kidneys and the urinary bladder harbour specialized parasites, monogeneans of the family Polystomatidae. 
The kidneys of reptiles consist of a number of lobules arranged in a broadly linear pattern. Each lobule contains a single branch of the ureter in its centre, into which the collecting ducts empty. Reptiles have relatively few nephrons compared with other amniotes of a similar size, possibly because of their lower metabolic rate. 
Birds have relatively large, elongated kidneys, each of which is divided into three or more distinct lobes. The lobes consists of several small, irregularly arranged, lobules, each centred on a branch of the ureter. Birds have small glomeruli, but about twice as many nephrons as similarly sized mammals. 
The human kidney is fairly typical of that of mammals. Distinctive features of the mammalian kidney, in comparison with that of other vertebrates, include the presence of the renal pelvis and renal pyramids and a clearly distinguishable cortex and medulla. The latter feature is due to the presence of elongated loops of Henle; these are much shorter in birds, and not truly present in other vertebrates (although the nephron often has a short intermediate segment between the convoluted tubules). It is only in mammals that the kidney takes on its classical "kidney" shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds and cetaceans. 
Refraction of light can be seen in many places in our everyday life. It makes objects under a water surface appear closer than they really are. It is what optical lenses are based on, allowing for instruments such as glasses, cameras, binoculars, microscopes, and the human eye. Refraction is also responsible for some natural optical phenomena including rainbows and mirages. 
A correct explanation of refraction involves two separate parts, both a result of the wave nature of light. 
Light slows as it travels through a medium other than vacuum (such as air, glass or water). This is not because of scattering or absorption. Rather it is because, as an electromagnetic oscillation, light itself causes other electrically charged particles such as electrons, to oscillate. The oscillating electrons emit their own electromagnetic waves which interact with the original light. The resulting "combined" wave has wave packets that pass an observer at a slower rate. The light has effectively been slowed. When light returns to a vacuum and there are no electrons nearby, this slowing effect ends and its speed returns to c. 
When light enters, exits or changes the medium it travels in, at an angle, one side or the other of the wavefront is slowed before the other. This asymmetrical slowing of the light causes it to change the angle of its travel. Once light is within the new medium with constant properties, it travels in a straight line again. 
As described above, the speed of light is slower in a medium other than vacuum. This slowing applies to any medium such as air, water, or glass, and is responsible for phenomena such as refraction. When light leaves the medium and returns to a vacuum, and ignoring any effects of gravity, its speed returns to the usual speed of light in a vacuum, c. 
Common explanations for this slowing, based upon the idea of light scattering from, or being absorbed and re-emitted by atoms, are both incorrect. Explanations like these would cause a "blurring" effect in the resulting light, as it would no longer be travelling in just one direction. But this effect is not seen in nature. 
A more correct explanation rests on light's nature as an electromagnetic wave. Because light is an oscillating electrical/magnetic wave, light traveling in a medium causes the electrically charged electrons of the material to also oscillate. (The material's protons also oscillate but as they are around 2000 times more massive, their movement and therefore their effect, is far smaller). A moving electrical charge emits electromagnetic waves of its own. The electromagnetic waves emitted by the oscillating electrons, interact with the electromagnetic waves that make up the original light, similar to water waves on a pond, a process known as constructive interference. When two waves interfere in this way, the resulting "combined" wave may have wave packets that pass an observer at a slower rate. The light has effectively been slowed. When the light leaves the material, this interaction with electrons no longer happens, and therefore the wave packet rate (and therefore its speed) return to normal. 
Consider a wave going from one material to another where its speed is slower as in the figure. If it reaches the interface between the materials at an angle one side of the wave will reach the second material first, and therefore slow down earlier. With one side of the wave going slower the whole wave will pivot towards that side. This is why a wave will bend away from the surface or toward the normal when going into a slower material. In the opposite case of a wave reaching a material where the speed is higher, one side of the wave will speed up and the wave will pivot away from that side. 
Another way of understanding the same thing is to consider the change in wavelength at the interface. When the wave goes from one material to another where the wave has a different speed v, the frequency f of the wave will stay the same, but the distance between wavefronts or wavelength λ=v/f will change. If the speed is decreased, such as in the figure to the right, the wavelength will also decrease. With an angle between the wave fronts and the interface and change in distance between the wave fronts the angle must change over the interface to keep the wave fronts intact. From these considerations the relationship between the angle of incidence θ1, angle of transmission θ2 and the wave speeds v1 and v2 in the two materials can be derived. This is the law of refraction or Snell's law and can be written as
Refraction occurs when light goes through a water surface since water has a refractive index of 1. 33 and air has a refractive index of about 1. Looking at a straight object, such as a pencil in the figure here, which is placed at a slant, partially in the water, the object appears to bend at the water's surface. This is due to the bending of light rays as they move from the water to the air. Once the rays reach the eye, the eye traces them back as straight lines (lines of sight). The lines of sight (shown as dashed lines) intersect at a higher position than where the actual rays originated. This causes the pencil to appear higher and the water to appear shallower than it really is. 
The depth that the water appears to be when viewed from above is known as the apparent depth. This is an important consideration for spearfishing from the surface because it will make the target fish appear to be in a different place, and the fisher must aim lower to catch the fish. Conversely, an object above the water has a higher apparent height when viewed from below the water. The opposite correction must be made by an archer fish. 
For small angles of incidence (measured from the normal, when sin θ is approximately the same as tan θ), the ratio of apparent to real depth is the ratio of the refractive indexes of air to that of water. But, as the angle of incidence approaches 90o, the apparent depth approaches zero, albeit reflection increases, which limits observation at high angles of incidence. Conversely, the apparent height approaches infinity as the angle of incidence (from below) increases, but even earlier, as the angle of total internal reflection is approached, albeit the image also fades from view as this limit is approached. 
Refraction is also responsible for rainbows and for the splitting of white light into a rainbow-spectrum as it passes through a glass prism. Glass has a higher refractive index than air. When a beam of white light passes from air into a material having an index of refraction that varies with frequency, a phenomenon known as dispersion occurs, in which different coloured components of the white light are refracted at different angles, i. e. , they bend by different amounts at the interface, so that they become separated. The different colors correspond to different frequencies. 
Throughout the 19th century the demand for nitrates and ammonia for use as fertilizers and industrial feedstocks had been steadily increasing. The main source was mining niter deposits and guano from tropical islands. At the beginning of the 20th century it was being predicted that these reserves could not satisfy future demands, and research into new potential sources of ammonia became more important. Although atmospheric nitrogen (N2) is abundant, comprising nearly 80% of the air, it is exceptionally stable and does not readily react with other chemicals. Converting N2 into ammonia posed a challenge for chemists globally. 
Haber, with his assistant Robert Le Rossignol, developed the high-pressure devices and catalysts needed to demonstrate the Haber process at laboratory scale. They demonstrated their process in the summer of 1909 by producing ammonia from air, drop by drop, at the rate of about 125 mL (4 US fl oz) per hour. The process was purchased by the German chemical company BASF, which assigned Carl Bosch the task of scaling up Haber's tabletop machine to industrial-level production. He succeeded in 1910. Haber and Bosch were later awarded Nobel prizes, in 1918 and 1931 respectively, for their work in overcoming the chemical and engineering problems of large-scale, continuous-flow, high-pressure technology. 
Ammonia was first manufactured using the Haber process on an industrial scale in 1913 in BASF's Oppau plant in Germany, reaching 20 tonnes per day the following year. During World War I, the production of munitions required large amounts of nitrate. The Allies had access to large deposits of sodium nitrate in Chile (Chile saltpetre) controlled by British companies. Germany had no such resources, so the Haber process proved essential to the German war effort. Synthetic ammonia from the Haber process was used for the production of nitric acid, a precursor to the nitrates used in explosives. 
Today, the most popular catalysts are based on iron promoted with K2O, CaO, SiO2, and Al2O3. Earlier, molybdenum was also used as a promoter. The original Haber–Bosch reaction chambers used osmium as the catalyst, but it was available in extremely small quantities. Haber noted uranium was almost as effective and easier to obtain than osmium. Under Bosch's direction in 1909, the BASF researcher Alwin Mittasch discovered a much less expensive iron-based catalyst, which is still used today. 
During the interwar years, alternative processes were developed, the most notably different being the Casale process and Claude process. Luigi Casale and Georges Claude proposed to increase the pressure of the synthesis loop to 80–100 MPa (800–1,000 bar; 12,000–15,000 psi), thereby increasing the single-pass ammonia conversion and making nearly complete liquefaction at ambient temperature feasible. Georges Claude even proposed to have three or four converters with liquefaction steps in series, thereby omitting the need for a recycle. Nowadays, most plants resemble the original Haber process (20 MPa (200 bar; 2,900 psi) and 500 °C (932 °F)), albeit with improved single-pass conversion and lower energy consumption due to process and catalyst optimization. 
A major contributor to the elucidation of this mechanism [clarification needed] was Gerhard Ertl. 
This conversion is typically conducted at pressures above 10 MPa (100 bar; 1,450 psi) and between 400 and 500 °C (752 and 932 °F), as the gases (nitrogen and hydrogen) are passed over four beds of catalyst, with cooling between each pass for maintaining a reasonable equilibrium constant. On each pass only about 15% conversion occurs, but any unreacted gases are recycled, and eventually an overall conversion of 97% is achieved. 
The steam reforming, shift conversion, carbon dioxide removal, and methanation steps each operate at pressures of about 2. 5–3. 5 MPa (25–35 bar; 360–510 psi), and the ammonia synthesis loop operates at pressures ranging from 6 to 18 MPa (60 to 180 bar; 870 to 2,610 psi), depending upon which proprietary process is used. 
The major source of hydrogen is methane from natural gas. The conversion, steam reforming, is conducted with steam in a high-temperature and -pressure tube inside a reformer with a nickel catalyst, separating the carbon and hydrogen atoms in the natural gas. Other fossil fuel sources include coal, heavy fuel oil and naphtha, while hydrogen is also produced from biomass and from electrolysis of water. 
Nitrogen gas (N2) is very unreactive because the atoms are held together by strong triple bonds. The Haber process relies on catalysts that accelerate the scission of this triple bond. 
Two opposing considerations are relevant to this synthesis: the position of the equilibrium and the rate of reaction. At room temperature, the equilibrium is strongly in favor of ammonia, but the reaction doesn't proceed at a detectable rate due to its high activation energy. Because the reaction is exothermic, the equilibrium constant becomes 1 around 150–200 °C (302–392 °F) (see Le Châtelier's principle). 
Above this temperature, the equilibrium quickly becomes quite unfavorable for the reaction product at atmospheric pressure, according to the van 't Hoff equation. Lowering the temperature is also unhelpful because the catalyst requires a temperature of at least 400 °C to be efficient. 
Increased pressure does favor the forward reaction because there are 4 moles of reactant for every 2 moles of product, and the pressure used (15–25 MPa (150–250 bar; 2,200–3,600 psi)) alters the equilibrium concentrations to give a substantial ammonia yield. The reason for this is evident in the equilibrium relationship, which is
where {\displaystyle {\hat {\phi }}_{i}} is the fugacity coefficient of species i, y_{i} is the mole fraction of the same species, P is the pressure in the reactor, and {\displaystyle P^{\circ }} is standard pressure, typically 1 bar (0. 10 MPa). 
Economically, pressurization of the reactor is expensive: pipes, valves, and reaction vessels need to be strengthened, and there are safety considerations when working at 20 MPa. In addition, running compressors takes considerable energy, as work must be done on the (very compressible) gas. Thus, the compromise used gives a single-pass yield of around 15%
While removing the product (i. e. , ammonia gas) from the system would increase the reaction yield, this step is not used in practice, since the temperature is too high; it is removed from the equilibrium mixture of gases leaving the reaction vessel. The hot gases are cooled enough, whilst maintaining a high pressure, for the ammonia to condense and be removed as liquid. Unreacted hydrogen and nitrogen gases are then returned to the reaction vessel to undergo further reaction. While most ammonia is removed (typically down to 2–5 mol. %), some ammonia remains in the recycle stream to the converter. In academic literature, more complete separation of ammonia has been proposed by absorption in metal halides and by adsorption on zeolites. Such a process is called a absorbent-enhanced Haber process or adsorbent-enhanced Haber-Bosch process. 
The theory of relativity usually encompasses two interrelated theories by Albert Einstein: special relativity and general relativity. Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to other forces of nature. It applies to the cosmological and astrophysical realm, including astronomy. 
The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton. It introduced concepts including spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes, and gravitational waves. 
Albert Einstein published the theory of special relativity in 1905, building on many theoretical results and empirical findings obtained by Albert A. Michelson, Hendrik Lorentz, Henri Poincaré and others. Max Planck, Hermann Minkowski and others did subsequent work. 
Einstein developed general relativity between 1907 and 1915, with contributions by many others after 1915. The final form of general relativity was published in 1916. 
The term "theory of relativity" was based on the expression "relative theory" (German: Relativtheorie) used in 1906 by Planck, who emphasized how the theory uses the principle of relativity. In the discussion section of the same paper, Alfred Bucherer used for the first time the expression "theory of relativity" (German: Relativitätstheorie). 
By the 1920s, the physics community understood and accepted special relativity. It rapidly became a significant and necessary tool for theorists and experimentalists in the new fields of atomic physics, nuclear physics, and quantum mechanics. 
By comparison, general relativity did not appear to be as useful, beyond making minor corrections to predictions of Newtonian gravitation theory. It seemed to offer little potential for experimental test, as most of its assertions were on an astronomical scale. Its mathematics seemed difficult and fully understandable only by a small number of people. Around 1960, general relativity became central to physics and astronomy. New mathematical techniques to apply to general relativity streamlined calculations and made its concepts more easily visualized. As astronomical phenomena were discovered, such as quasars (1963), the 3-kelvin microwave background radiation (1965), pulsars (1967), and the first black hole candidates (1981), the theory explained their attributes, and measurement of them further confirmed the theory. 
Special relativity is a theory of the structure of spacetime. It was introduced in Einstein's 1905 paper "On the Electrodynamics of Moving Bodies" (for the contributions of many other physicists see History of special relativity). Special relativity is based on two postulates which are contradictory in classical mechanics:
The laws of physics are the same for all observers in any inertial frame of reference relative to one another (principle of relativity). 
The speed of light in a vacuum is the same for all observers, regardless of their relative motion or of the motion of the light source. 
The resultant theory copes with experiment better than classical mechanics. For instance, postulate 2 explains the results of the Michelson–Morley experiment. Moreover, the theory has many surprising and counterintuitive consequences. Some of these are:
Relativity of simultaneity: Two events, simultaneous for one observer, may not be simultaneous for another observer if the observers are in relative motion. 
Time dilation: Moving clocks are measured to tick more slowly than an observer's "stationary" clock. 
Length contraction: Objects are measured to be shortened in the direction that they are moving with respect to the observer. 
Maximum speed is finite: No physical object, message or field line can travel faster than the speed of light in a vacuum. 
The effect of gravity can only travel through space at the speed of light, not faster or instantaneously. 
Mass–energy equivalence: E = mc2, energy and mass are equivalent and transmutable. 
Relativistic mass, idea used by some researchers. 
The defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics by the Lorentz transformations. (See Maxwell's equations of electromagnetism. )
General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics. This is incompatible with classical mechanics and special relativity because in those theories inertially moving objects cannot accelerate with respect to each other, but objects in free fall do so. To resolve this difficulty Einstein first proposed that spacetime is curved. In 1915, he devised the Einstein field equations which relate the curvature of spacetime with the mass, energy, and any momentum within it. 
Some of the consequences of general relativity are:
Gravitational time dilation: Clocks run slower in deeper gravitational wells. 
Precession: Orbits precess in a way unexpected in Newton's theory of gravity. (This has been observed in the orbit of Mercury and in binary pulsars). 
Light deflection: Rays of light bend in the presence of a gravitational field. 
Frame-dragging: Rotating masses "drag along" the spacetime around them. 
Metric expansion of space: the universe is expanding, and the far parts of it are moving away from us faster than the speed of light. 
Technically, general relativity is a theory of gravitation whose defining feature is its use of the Einstein field equations. The solutions of the field equations are metric tensors which define the topology of the spacetime and how objects move inertially. 
Einstein stated that the theory of relativity belongs to a class of "principle-theories". As such, it employs an analytic method, which means that the elements of this theory are not based on hypothesis but on empirical discovery. By observing natural processes, we understand their general characteristics, devise mathematical models to describe what we observed, and by analytical means we deduce the necessary conditions that have to be satisfied. Measurement of separate events must satisfy these conditions and match the theory's conclusions. 
Relativity is a falsifiable theory: It makes predictions that can be tested by experiment. In the case of special relativity, these include the principle of relativity, the constancy of the speed of light, and time dilation. The predictions of special relativity have been confirmed in numerous tests since Einstein published his paper in 1905, but three experiments conducted between 1881 and 1938 were critical to its validation. These are the Michelson–Morley experiment, the Kennedy–Thorndike experiment, and the Ives–Stilwell experiment. Einstein derived the Lorentz transformations from first principles in 1905, but these three experiments allow the transformations to be induced from experimental evidence. 
Maxwell's equations—the foundation of classical electromagnetism—describe light as a wave that moves with a characteristic velocity. The modern view is that light needs no medium of transmission, but Maxwell and his contemporaries were convinced that light waves were propagated in a medium, analogous to sound propagating in air, and ripples propagating on the surface of a pond. This hypothetical medium was called the luminiferous aether, at rest relative to the "fixed stars" and through which the Earth moves. Fresnel's partial ether dragging hypothesis ruled out the measurement of first-order (v/c) effects, and although observations of second-order effects (v2/c2) were possible in principle, Maxwell thought they were too small to be detected with then-current technology. 
The Michelson–Morley experiment was designed to detect second-order effects of the "aether wind"—the motion of the aether relative to the earth. Michelson designed an instrument called the Michelson interferometer to accomplish this. The apparatus was more than accurate enough to detect the expected effects, but he obtained a null result when the first experiment was conducted in 1881, and again in 1887. Although the failure to detect an aether wind was a disappointment, the results were accepted by the scientific community. In an attempt to salvage the aether paradigm, FitzGerald and Lorentz independently created an ad hoc hypothesis in which the length of material bodies changes according to their motion through the aether. This was the origin of FitzGerald–Lorentz contraction, and their hypothesis had no theoretical basis. The interpretation of the null result of the Michelson–Morley experiment is that the round-trip travel time for light is isotropic (independent of direction), but the result alone is not enough to discount the theory of the aether or validate the predictions of special relativity. 
While the Michelson–Morley experiment showed that the velocity of light is isotropic, it said nothing about how the magnitude of the velocity changed (if at all) in different inertial frames. The Kennedy–Thorndike experiment was designed to do that, and was first performed in 1932 by Roy Kennedy and Edward Thorndike. They obtained a null result, and concluded that "there is no effect . . . unless the velocity of the solar system in space is no more than about half that of the earth in its orbit". That possibility was thought to be too coincidental to provide an acceptable explanation, so from the null result of their experiment it was concluded that the round-trip time for light is the same in all inertial reference frames. 
The Ives–Stilwell experiment was carried out by Herbert Ives and G. R. Stilwell first in 1938 and with better accuracy in 1941. It was designed to test the transverse Doppler effect – the redshift of light from a moving source in a direction perpendicular to its velocity—which had been predicted by Einstein in 1905. The strategy was to compare observed Doppler shifts with what was predicted by classical theory, and look for a Lorentz factor correction. Such a correction was observed, from which was concluded that the frequency of a moving atomic clock is altered according to special relativity. 
Those classic experiments have been repeated many times with increased precision. Other experiments include, for instance, relativistic energy and momentum increase at high velocities, experimental testing of time dilation, and modern searches for Lorentz violations. 
Far from being simply of theoretical interest, relativistic effects are important practical engineering concerns. Satellite-based measurement needs to take into account relativistic effects, as each satellite is in motion relative to an Earth-bound user and is thus in a different frame of reference under the theory of relativity. Global positioning systems such as GPS, GLONASS, and Galileo, must account for all of the relativistic effects, such as the consequences of Earth's gravitational field, in order to work with precision. This is also the case in the high-precision measurement of time. Instruments ranging from electron microscopes to particle accelerators would not work if relativistic considerations were omitted. 
The spacetime symmetry group for Special Relativity is the Poincaré group, which is a ten-dimensional group of three Lorentz boosts, three rotations, and four spacetime translations. It is logical to ask what symmetries if any might apply in General Relativity. A tractable case may be to consider the symmetries of spacetime as seen by observers located far away from all sources of the gravitational field. The naive expectation for asymptotically flat spacetime symmetries might be simply to extend and reproduce the symmetries of flat spacetime of special relativity, viz. , the Poincaré group. 
In 1962 Hermann Bondi, M. G. van der Burg, A. W. Metzner and Rainer K. Sachs addressed this asymptotic symmetry problem in order to investigate the flow of energy at infinity due to propagating gravitational waves. Their first step was to decide on some physically sensible boundary conditions to place on the gravitational field at light-like infinity to characterize what it means to say a metric is asymptotically flat, making no a priori assumptions about the nature of the asymptotic symmetry group — not even the assumption that such a group exists. Then after designing what they considered to be the most sensible boundary conditions, they investigated the nature of the resulting asymptotic symmetry transformations that leave invariant the form of the boundary conditions appropriate for asymptotically flat gravitational fields. What they found was that the asymptotic symmetry transformations actually do form a group and the structure of this group does not depend on the particular gravitational field that happens to be present. This means that, as expected, one can separate the kinematics of spacetime from the dynamics of the gravitational field at least at spatial infinity. The puzzling surprise in 1962 was their discovery of a rich infinite-dimensional group (the so-called BMS group) as the asymptotic symmetry group, instead of the finite-dimensional Poincaré group, which is a subgroup of the BMS group. Not only are the Lorentz transformations asymptotic symmetry transformations, there are also additional transformations that are not Lorentz transformations but are asymptotic symmetry transformations. In fact, they found an additional infinity of transformation generators known as supertranslations. This implies the conclusion that General Relativity does not reduce to special relativity in the case of weak fields at long distances. 
A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov. 
Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics. 
Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. 
The adjective Markovian is used to describe something that is related to a Markov process. 
A Markov process is a stochastic process that satisfies the Markov property (sometimes characterized as "memorylessness"). In simpler terms, it is a process for which predictions can be made regarding future outcomes based solely on its present state and—most importantly—such predictions are just as good as the ones that could be made knowing the process's full history. In other words, conditional on the present state of the system, its future and past states are independent. 
A Markov chain is a type of Markov process that has either a discrete state space or a discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space). 
Markov studied Markov processes in the early 20th century, publishing his first paper on the topic in 1906. Markov processes in continuous time were discovered long before Andrey Markov's work in the early 20th century in the form of the Poisson process. Markov was interested in studying an extension of independent random sequences, motivated by a disagreement with Pavel Nekrasov who claimed independence was necessary for the weak law of large numbers to hold. In his first paper on Markov chains, published in 1906, Markov showed that under certain conditions the average outcomes of the Markov chain would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption, which had been commonly regarded as a requirement for such mathematical laws to hold. Markov later used Markov chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains. 
In 1912 Henri Poincaré studied Markov chains on finite groups with an aim to study card shuffling. Other early uses of Markov chains include a diffusion model, introduced by Paul and Tatyana Ehrenfest in 1907, and a branching process, introduced by Francis Galton and Henry William Watson in 1873, preceding the work of Markov. After the work of Galton and Watson, it was later revealed that their branching process had been independently discovered and studied around three decades earlier by Irénée-Jules Bienaymé. Starting in 1928, Maurice Fréchet became interested in Markov chains, eventually resulting in him publishing in 1938 a detailed study on Markov chains. 
Andrei Kolmogorov developed in a 1931 paper a large part of the early theory of continuous-time Markov processes. Kolmogorov was partly inspired by Louis Bachelier's 1900 work on fluctuations in the stock market as well as Norbert Wiener's work on Einstein's model of Brownian movement. He introduced and studied a particular set of Markov processes known as diffusion processes, where he derived a set of differential equations describing the processes. Independent of Kolmogorov's work, Sydney Chapman derived in a 1928 paper an equation, now called the Chapman–Kolmogorov equation, in a less mathematically rigorous way than Kolmogorov, while studying Brownian movement. The differential equations are now called the Kolmogorov equations or the Kolmogorov–Chapman equations. Other mathematicians who contributed significantly to the foundations of Markov processes include William Feller, starting in 1930s, and then later Eugene Dynkin, starting in the 1950s. 
Random walks based on integers and the gambler's ruin problem are examples of Markov processes. Some variations of these processes were studied hundreds of years earlier in the context of independent variables. Two important examples of Markov processes are the Wiener process, also known as the Brownian motion process, and the Poisson process, which are considered the most important and central stochastic processes in the theory of stochastic processes. These two processes are Markov processes in continuous time, while random walks on the integers and the gambler's ruin problem are examples of Markov processes in discrete time. 
A famous Markov chain is the so-called "drunkard's walk", a random walk on the number line where, at each step, the position may change by +1 or −1 with equal probability. From any position there are two possible transitions, to the next or previous integer. The transition probabilities depend only on the current position, not on the manner in which the position was reached. For example, the transition probabilities from 5 to 4 and 5 to 6 are both 0. 5, and all other transition probabilities from 5 are 0. These probabilities are independent of whether the system was previously in 4 or 6. 
Another example is the dietary habits of a creature that eats only grapes, cheese, or lettuce, and whose dietary habits conform to the following rules:
It eats exactly once a day. 
If it ate cheese today, tomorrow it will eat lettuce or grapes with equal probability. 
If it ate grapes today, tomorrow it will eat grapes with probability 1/10, cheese with probability 4/10, and lettuce with probability 5/10. 
If it ate lettuce today, tomorrow it will eat grapes with probability 4/10 or cheese with probability 6/10. It will not eat lettuce again tomorrow. 
This creature's eating habits can be modeled with a Markov chain since its choice tomorrow depends solely on what it ate today, not what it ate yesterday or any other time in the past. One statistical property that could be calculated is the expected percentage, over a long period, of the days on which the creature will eat grapes. 
A series of independent events (for example, a series of coin flips) satisfies the formal definition of a Markov chain. However, the theory is usually applied only when the probability distribution of the next step depends non-trivially on the current state. 
Digestion is the breakdown of large insoluble food molecules into small water-soluble food molecules so that they can be absorbed into the watery blood plasma. In certain organisms, these smaller substances are absorbed through the small intestine into the blood stream. Digestion is a form of catabolism that is often divided into two processes based on how food is broken down: mechanical and chemical digestion. The term mechanical digestion refers to the physical breakdown of large pieces of food into smaller pieces which can subsequently be accessed by digestive enzymes. In chemical digestion, enzymes break down food into the small molecules the body can use. 
In the human digestive system, food enters the mouth and mechanical digestion of the food starts by the action of mastication (chewing), a form of mechanical digestion, and the wetting contact of saliva. Saliva, a liquid secreted by the salivary glands, contains salivary amylase, an enzyme which starts the digestion of starch in the food; the saliva also contains mucus, which lubricates the food, and hydrogen carbonate, which provides the ideal conditions of pH (alkaline) for amylase to work. After undergoing mastication and starch digestion, the food will be in the form of a small, round slurry mass called a bolus. It will then travel down the esophagus and into the stomach by the action of peristalsis. Gastric juice in the stomach starts protein digestion. Gastric juice mainly contains hydrochloric acid and pepsin. In infants and toddlers gastric juice also contains rennin. As the first two chemicals may damage the stomach wall, mucus is secreted by the stomach, providing a slimy layer that acts as a shield against the damaging effects of the chemicals. At the same time protein digestion is occurring, mechanical mixing occurs by peristalsis, which is waves of muscular contractions that move along the stomach wall. This allows the mass of food to further mix with the digestive enzymes. Studies suggest that increasing the number of chews per bite increases relevant gut hormones and may decrease self-reported hunger and food intake. 
After some time (typically 1–2 hours in humans, 4–6 hours in dogs, 3–4 hours in house cats),[citation needed] the resulting thick liquid is called chyme. When the pyloric sphincter valve opens, chyme enters the duodenum where it mixes with digestive enzymes from the pancreas and bile juice from the liver and then passes through the small intestine, in which digestion continues. When the chyme is fully digested, it is absorbed into the blood. 95% of nutrient absorption occurs in the small intestine. Water and minerals are reabsorbed back into the blood in the colon (large intestine) where the pH is slightly acidic about 5. 6 ~ 6. 9. Some vitamins, such as biotin and vitamin K (K2MK7) produced by bacteria in the colon are also absorbed into the blood in the colon. Waste material is eliminated from the rectum during defecation. 
In physics, spacetime is any mathematical model which fuses the three dimensions of space and the one dimension of time into a single four-dimensional manifold. The fabric of space-time is a conceptual model combining the three dimensions of space with the fourth dimension of time. Spacetime diagrams can be used to visualize relativistic effects, such as why different observers perceive differently where and when events occur. 
Until the 20th century, it was assumed that the three-dimensional geometry of the universe (its spatial expression in terms of coordinates, distances, and directions) was independent of one-dimensional time. The famous physicist Albert Einstein helped develop the idea of space-time as part of his theory of relativity. Prior to his pioneering work, scientists had two separate theories to explain physical phenomena: Isaac Newton's laws of physics described the motion of massive objects, while James Clerk Maxwell's electromagnetic models explained the properties of light. However, in 1905, Albert Einstein based a work on special relativity on two postulates:
The laws of physics are invariant (i. e. , identical) in all inertial systems (i. e. , non-accelerating frames of reference)
The speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. 
The logical consequence of taking these postulates together is the inseparable joining together of the four dimensions—hitherto assumed as independent—of space and time. Many counterintuitive consequences emerge: in addition to being independent of the motion of the light source, the speed of light is constant regardless of the frame of reference in which it is measured; the distances and even temporal ordering of pairs of events change when measured in different inertial frames of reference (this is the relativity of simultaneity); and the linear additivity of velocities no longer holds true. 
Einstein framed his theory in terms of kinematics (the study of moving bodies). His theory was an advance over Lorentz's 1904 theory of electromagnetic phenomena and Poincaré's electrodynamic theory. Although these theories included equations identical to those that Einstein introduced (i. e. , the Lorentz transformation), they were essentially ad hoc models proposed to explain the results of various experiments—including the famous Michelson–Morley interferometer experiment—that were extremely difficult to fit into existing paradigms. 
In 1908, Hermann Minkowski—once one of the math professors of a young Einstein in Zürich—presented a geometric interpretation of special relativity that fused time and the three spatial dimensions of space into a single four-dimensional continuum now known as Minkowski space. A key feature of this interpretation is the formal definition of the spacetime interval. Although measurements of distance and time between events differ for measurements made in different reference frames, the spacetime interval is independent of the inertial frame of reference in which they are recorded. 
Minkowski's geometric interpretation of relativity was to prove vital to Einstein's development of his 1915 general theory of relativity, wherein he showed how mass and energy curve flat spacetime into a pseudo-Riemannian manifold. 